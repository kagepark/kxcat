####################################
# License : GPL
####################################
error_exit() {
     echo "$*"
     exit 1
}


if [ -f /etc/profile.d/slurm.sh ]; then
    source /etc/profile.d/slurm.sh
    [ -f $localstatedir/run/slurmctld.pid ] || error_exit "Not running SLURM"
    pid=$(< $localstatedir/run/slurmctld.pid)
    if [ ! -d /proc/$pid ]; then
       echo "Not running SLURM"
       exit
    fi
else
    echo "Not ready SLURM"
    exit 
fi

slurm_conf=$sysconfdir/slurm.conf
qstat() {
   if [ "$1" == "-h" ]; then
      echo "
${FUNCNAME}              : View queue status
${FUNCNAME} -f <job id>  : Display Queue's detail information
${FUNCNAME} -q           : Show simple queue summary
${FUNCNAME} -l           : Show start queue time
${FUNCNAME} -r           : Show running queue list
      "
      exit
   elif [ "$1" == "-q" ]; then
      sjstat -c
   elif [ "$1" == "-l" ]; then
      sinfo
      sjstat -v
   elif [ "$1" == "-r" ]; then
      sjstat -r
   elif [ "$1" == "-f" ]; then
      [ -n "$2" ] || error_exit "${FUNCNAME} -f <job id>"
      scontrol show job $2
   else
      sinfo
      sjstat
   fi
}

qsub() {
   batch_file=$(echo $* | sed "s/ /\n/g"| tail -n1)
   [ -n "$batch_file" ] || error_exit "${FUNCNAME} [<options>...] <batch file>"
   [ -f "$batch_file" ] || error_exit "$batch_file not found"
   sbatch $*
}

qdel() {
   local job_id
   job_id=$1
   [ -n "$job_id" ] || error_exit "${FUNCNAME} <job id>"
   scancel $job_id
}


slurm_restart() {
  local slurm_nodes
  slurm_nodes=$(echo $* | sed "s/ /,/g")
  if [ -f /lib/systemd/system/slurmctld.service ]; then
     systemctl restart slurmctld 
     systemctl restart slurmd
  else
     /etc/init.d/slurm restart
  fi
  [ -n "$slurm_nodes" ] || slurm_nodes=$(kxcat nodes | grep " booted" | awk '{printf "%s,",$2}' | sed "s/,$//g")
  if [ -n "$slurm_nodes" ]; then
     xdcp $slurm_nodes /etc/slurm/slurm.conf /etc/slurm/slurm.conf
     xdsh $slurm_nodes "[ -f /lib/systemd/system/slurmd.service ] && systemctl restart slurmd || /etc/init.d/slurm restart"
  fi
}

add_nodes() {
    local partition group proc_num
    partition=$1
    nodes=$2
    proc_num=$3
    [ -n "$proc_num" ] || proc_num=$proc_num_default
    [ ! -n "$partition" -o ! -n "$nodes" ] && error_exit  "${FUNCNAME} <queue name> <group name> [<proc #>]"

    partition_info=$(grep "^PartitionName=$partition" $slurm_conf)
    if [ -n "$partition_info" ]; then
        node_info="$(for ii in $partition_info; do echo $ii | awk -F= '{if($1 == "Nodes") print $2}'; done),$nodes"
        sed -i "s/$partition_info/PartitionName=$partition Nodes=$node_info Default=YES MaxTime=INFINITE State=UP/g" $slurm_conf
    else
        echo "Not found $partition"
    fi
}

del_nodes() {
    local partition group proc_num
    partition=$1
    nodes=$2
    proc_num=$3
    [ -n "$proc_num" ] || proc_num=$proc_num_default
    [ ! -n "$partition" -o ! -n "$nodes" ] && error_exit  "${FUNCNAME} <queue name> <group name> [<proc #>]"

    partition_info=$(grep "^PartitionName=$partition" $slurm_conf)
    if [ -n "$partition_info" ]; then
        node_info=($(for ii in $partition_info; do echo $ii | awk -F= '{if($1 == "Nodes") print $2}' | sed "s/,/ /g"; done))
        split=0
        new_node_info=$(for ((ii=0; ii<${#node_info[*]}; ii++)); do
            [ "${node_info[$ii]}" == "$nodes" ] && continue
            if [ "$split" == "0" ]; then
                echo -n ${node_info[$ii]} 
                split=1
            else
                echo -n ",${node_info[$ii]}"
            fi
        done)
        sed -i "s/$partition_info/PartitionName=$partition Nodes=$new_node_info Default=YES MaxTime=INFINITE State=UP/g" $slurm_conf
        
    else
        echo "Not found $partition"
    fi
}


del_queue() {
    local partition proc_num line
    partition=$1
    [ ! -n "$partition" ] && error_exit  "${FUNCNAME} <partition>"
    if grep "^PartitionName=$partition " $slurm_conf >& /dev/null; then
       sed -i "/^PartitionName=$partition /d" $slurm_conf
    else
       echo "$partition not found"
    fi
}

add_queue() {
    local partition proc_num line
    partition=$1
    nodes=$2
    [ ! -n "$partition" -o ! -n "$nodes" ] && error_exit  "${FUNCNAME} <partition> <node list>"
    if grep "^PartitionName=$partition " $slurm_conf >& /dev/null; then
        echo "Alread added $partition"
    else
        echo "PartitionName=$partition Nodes=$nodes Default=YES MaxTime=INFINITE State=UP" >> $slurm_conf
    fi
}

slurm_drain() {
    local nodes reason
    nodes=$1
    reason=$2
    [ ! -n "$nodes" -o ! -n "$reason" ] && error_exit "${FUNCNAME} <node list> <reason>"
    scontrol update nodename=$nodes state=drain  Reason="$reason"
}

slurm_clean() {
    local nodes reason
    nodes=$1
    reason=$2
    [ ! -n "$nodes" ] && error_exit "${FUNCNAME} <node list> [<reason>]"
    if [ -n "$reason" ]; then
       scontrol update nodename=$nodes state=undrain  Reason="$reason"
    else
       scontrol update nodename=$nodes state=idle 
    fi
}

slurm_check() {
    local node 
    node=$1
    [ -n "$node" ] || error_exit "${FUNCNAME} <hostname>"
    xdsh $node "scontrol show slurmd"
}

queue_sample() {
    local qfile
    qfile=$1
    [ -n "$qfile" ] || error_exit "${FUNCNAME} <batch file>"
    [ -f "$qfile" ] && error_exit "$qfile found"
    #[ -n "$job_name" ] || job_name=$(awk -F: -v id=$(id -u) '{if($3==id) print $1}' /etc/passwd)_job
    echo "#!/bin/bash 
# #SBATCH --partition=<queue name>             # define running queue name (-p)
# #SBATCH --job-name=<job name>                # Job name (-J)
# #SBATCH --time=<time>                        # Define running time (-t) (days-hours:minutes:seconds)
# #SBATCH --nodes=<num>                        # Find available host number per node (-N), (same as : sbatch -N# <batch file>)
# #SBATCH --ntasks=<num>                       # Find available host number per core (-n)
# #SBATCH --chdir=<work dir>                   # define work dir (-D)
# #SBATCH --ntasks-per-node=8                  # take per node
# #SBATCH --mem=20GB                           # Memory define
# #SBATCH --constraint=IB                      # MPI jobs should request nodes that have InfiniBand
# #SBATCH -w, --nodelist=cpn-f16-35,cpn-f16-37 # Requesting Specific Nodes
# #SBATCH --exclude=cpn-f16-35,cpn-f16-37      # Excluding Specific Nodes
# #SBATCH --output=test-srun.%j.out            # Output file (-o)
# #SBATCH --error=test-srun.%j.err             # Error output file (-e)
#
###############################################################
## Module ##
# . /etc/profile.d/modules.sh
# module load <name>
# sleep 5
#
###############################################################
## Get hostname ##
# srun --nodes=\${SLURM_NNODES} bash -c 'hostname -s ' | sort -n > /tmp/slurm.node # get hostnames
## or ##
# srun --nodes=\${SLURM_NNODES} bash -c 'hostname -s ' | sort -u > /tmp/slurm.node # get uniq hostnames
#
###############################################################
## For OpenMP (4 OpenMP Thread) ##
# export OMP_NUM_THREADS=4
# ./mycode.exe
#
###############################################################
## Hybrid MPI/OpenMP (3 MPI tasks, 4 OpenMP Thread per task) ##
# export OMP_NUM_THREADS=4
# srun -n 3 -c 4 ./mycode.exe
#
###############################################################
## For MPI ##
## Total 64 process ##
# srun -n 64 -c 1 ./mycode.exe 
## or ##
# mpirun -np \$(cat /tmp/slurm.node | wc -l) -machinefile /tmp/slurm.node ./mycode.exe 
#
###############################################################
## Total 64 process (-c: cpus per task) ##
# srun -n 64 -c 4 ./mycode.exe
#
###############################################################
## The PMI library is necessary for srun ##
# export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so
# srun ./mycode.exe
#
###############################################################
## MPICH2 ##
# mpicc -L<path_to_slurm_lib> -lpmi â€¦
# srun -n20 ./mycode.exe
#
###############################################################
## MVAPICH ##
# srun -n16 --mpi=mvapich ./mycode.exe
#
###############################################################
## MVAPICH2 ##
# mpicc -L<path_to_slurm_lib> -lpmi ...
# srun -n16 --mpi=none ./mycode.exe
    " > $qfile
}
