####################################
# License : GPL
####################################
error_exit() {
     echo "$*"
     exit 1
}


if [ -f /etc/profile.d/slurm.sh ]; then
    source /etc/profile.d/slurm.sh
    pid=$(< $localstatedir/run/slurmctld.pid)
    if [ ! -d /proc/$pid ]; then
       echo "Not running SLURM"
       exit
    fi
else
    echo "Not ready SLURM"
    exit 
fi

slurm_conf=$sysconfdir/slurm.conf
qstat() {
   if [ "$1" == "-h" ]; then
      echo "
${FUNCNAME}              : View queue status
${FUNCNAME} -f <job id>  : Display Queue's detail information
${FUNCNAME} -q           : Show simple queue summary
${FUNCNAME} -l           : Show start queue time
${FUNCNAME} -r           : Show running queue list
      "
      exit
   elif [ "$1" == "-q" ]; then
      sjstat -c
   elif [ "$1" == "-l" ]; then
      sinfo
      sjstat -v
   elif [ "$1" == "-r" ]; then
      sjstat -r
   elif [ "$1" == "-f" ]; then
      [ -n "$2" ] || error_exit "${FUNCNAME} -f <job id>"
      scontrol show job $2
   else
      sinfo
      sjstat
   fi
}

qsub() {
   batch_file=$(echo $* | sed "s/ /\n/g"| tail -n1)
   [ -n "$batch_file" ] || error_exit "${FUNCNAME} [<options>...] <batch file>"
   [ -f "$batch_file" ] || error_exit "$batch_file not found"
   sbatch $*
}

qdel() {
   local job_id
   job_id=$1
   [ -n "$job_id" ] || error_exit "${FUNCNAME} <job id>"
   scancel $job_id
}


slurm_restart() {
  if [ -f /lib/systemd/system/slurmctld.service ]; then
     systemctl restart slurmctld 
     systemctl restart slurmd
  else
     /etc/init.d/slurm restart
  fi
  xdsh $(kxcat nodes | grep " booted" | awk '{printf "%s,",$2}') "[ -f /lib/systemd/system/slurmd.service ] && systemctl restart slurmd || /etc/init.d/slurm restart"
}

add_nodes() {
    local partition group proc_num
    partition=$1
    nodes=$2
    proc_num=$3
    [ -n "$proc_num" ] || proc_num=$proc_num_default
    [ ! -n "$partition" -o ! -n "$nodes" ] && error_exit  "${FUNCNAME} <queue name> <group name> [<proc #>]"

    partition_info=$(grep "^PartitionName=$partition" $slurm_conf)
    if [ -n "$partition_info" ]; then
        node_info="$(for ii in $partition_info; do echo $ii | awk -F= '{if($1 == "Nodes") print $2}'; done),$nodes"
        sed -i "s/$partition_info/PartitionName=$partition Nodes=$node_info Default=YES MaxTime=INFINITE State=UP/g" $slurm_conf
    else
        echo "Not found $partition"
    fi
}

del_nodes() {
    local partition group proc_num
    partition=$1
    nodes=$2
    proc_num=$3
    [ -n "$proc_num" ] || proc_num=$proc_num_default
    [ ! -n "$partition" -o ! -n "$nodes" ] && error_exit  "${FUNCNAME} <queue name> <group name> [<proc #>]"

    partition_info=$(grep "^PartitionName=$partition" $slurm_conf)
    if [ -n "$partition_info" ]; then
        node_info=($(for ii in $partition_info; do echo $ii | awk -F= '{if($1 == "Nodes") print $2}' | sed "s/,/ /g"; done))
        split=0
        new_node_info=$(for ((ii=0; ii<${#node_info[*]}; ii++)); do
            [ "${node_info[$ii]}" == "$nodes" ] && continue
            if [ "$split" == "0" ]; then
                echo -n ${node_info[$ii]} 
                split=1
            else
                echo -n ",${node_info[$ii]}"
            fi
        done)
        sed -i "s/$partition_info/PartitionName=$partition Nodes=$new_node_info Default=YES MaxTime=INFINITE State=UP/g" $slurm_conf
        
    else
        echo "Not found $partition"
    fi
}


del_queue() {
    local partition proc_num line
    partition=$1
    [ ! -n "$partition" ] && error_exit  "${FUNCNAME} <partition>"
    if grep "^PartitionName=$partition " $slurm_conf >& /dev/null; then
       sed -i "/^PartitionName=$partition /d" $slurm_conf
    else
       echo "$partition not found"
    fi
}

add_queue() {
    local partition proc_num line
    partition=$1
    nodes=$2
    [ ! -n "$partition" -o ! -n "$nodes" ] && error_exit  "${FUNCNAME} <partition> <node list>"
    if grep "^PartitionName=$partition " $slurm_conf >& /dev/null; then
        echo "Alread added $partition"
    else
        echo "PartitionName=$partition Nodes=$nodes Default=YES MaxTime=INFINITE State=UP" >> $slurm_conf
    fi
}

slurm_drain() {
    local nodes reason
    nodes=$1
    reason=$2
    [ ! -n "$nodes" -o ! -n "$reason" ] && error_exit "${FUNCNAME} <node list> <reason>"
    scontrol update nodename=$nodes state=drain  Reason="$reason"
}

slurm_clean() {
    local nodes reason
    nodes=$1
    reason=$2
    [ ! -n "$nodes" -o ! -n "$reason" ] && error_exit "${FUNCNAME} <node list> <reason>"
    scontrol update nodename=$nodes state=undrain  Reason="$reason"
}

queue_sample() {
    local qfile
    qfile=$1
    qname=$2
    num_nodes=$3
    time=$4
    job_name=$5
    [ -n "$qfile" ] || error_exit "${FUNCNAME} <batch file>"
    [ -f "$qfile" ] && error_exit "$qfile found"
    [ -n "$job_name" ] || job_name=$(awk -F: -v id=$(id -u) '{if($3==id) print $1}' /etc/passwd)_job
    echo "#!/bin/bash 
$([ -n "$qname" ] && echo "#SBATCH --partition=$qname")
$([ -n "$num_nodes" ] && echo "#SBATCH -N $num_nodes")
$([ -n "$time" ] && echo "#SBATCH -t $time")
#SBATCH --job-name=$job_name                   # Job name
# #SBATCH --ntasks-per-node=8                  # take per node
# #SBATCH --mem=20GB                           # Memory define
# #SBATCH --constraint=IB                      # MPI jobs should request nodes that have InfiniBand
# #SBATCH -w, --nodelist=cpn-f16-35,cpn-f16-37 # Requesting Specific Nodes
# #SBATCH --exclude=cpn-f16-35,cpn-f16-37      # Excluding Specific Nodes
# #SBATCH --output=test-srun.%j.out            # Output file
# #SBATCH --error=test-srun.%j.err             # Error output file
#
## For OpenMP (4 OpenMP Thread)
# export OMP_NUM_THREADS=4
# ./mycode.exe
#
## Hybrid MPI/OpenMP (3 MPI tasks, 4 OpenMP Thread per task)
# export OMP_NUM_THREADS=4
# srun -n 3 -c 4 ./mycode.exe
#
## Fure MPI
## Total 64 process
# srun -n 64 -c 1 ./mycode.exe 
## or
# srun 'hostname -s' | sort -u > /tmp/slurm.node
# mpirun -np ${SLURM_NODES} -machinefile /tmp/slurm.node ./mycode.exe 
#
#
## Total 64 process (-c: cpus per task)
# srun -n 64 -c 4 ./mycode.exe
    " > $qfile
}
